---
title: "Advanced spatial R workshop"
author: "Tobias Andermann"
date: "9/27/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


In this exercise we are going to test which predictors are spatially significantly correlated with a biological variable. For example you would apply the tools we are covering in this workshop if you want to test if species diversity in your studied organism group is significantly correlated with temperature, precipitation, predator diversity, etc. While this is a common operation in particular in marco-ecological/-evolutionary studies, there are several caveats that one needs to account for. Here we will deal with the main ones, which are namely raster projection, testing for and accounting for spatial autocorrelation and applying different models (namely GLM and SAR) for statistical testing.

In this scenario we will check which environmental predictors have a significant influence on mammal species diversity.


### The response variable
In our example mammal species diversity is the response variable, i.e. we want to figure out how species diversity responds to different predictors. You will need to produce the species diversity raster by adding up the presence/absence rasters belonging to each of the species of your mammal group of choice.

For selecting the species names belonging to your chosen group (order, family or genus), you can use the following taxonomy information available from the [Phylacine database](https://megapast2future.github.io/PHYLACINE_1.2/):

```{r, message=FALSE, warning=FALSE}
mammal_taxonomic_data = read.csv('../data/trait_data.csv')
```

For example you can extract all species from the order `Carnivora` like this:

```{r, message=FALSE, warning=FALSE}
species_list = mammal_taxonomic_data[mammal_taxonomic_data$Order.1.2 == 'Carnivora',]$Binomial.1.2
```

Now load the range files for these species and add them up for producing an overall global diversity map of the selected group. Also load a shape of a world map to crop out only land cells (using the `mask()` function). The final result should look something like this (in case you're stuck follow the instructions in [our basic spatial R tutorial](http://htmlpreview.github.io/?https://github.com/tobiashofmann88/workshops/blob/master/spatial_r_workshop/introduction_and_tutorials/tutorial_2.html)):

```{r, message=FALSE, warning=FALSE, echo=FALSE, cache = TRUE}
library(raster)
range_folder = '../../spatial_r_workshop/data/present_natural_converted'
species_ranges=list.files(range_folder,pattern = '*.grd')

raster = raster(paste0(range_folder,'/',species_list[1],'.grd'))
raster[raster>0] = 0

for (i in species_list){
  index = which(grepl(i, species_ranges))
  if (length(index)==0){
    no_match=TRUE
  }else{
  species_raster=raster(paste0(range_folder,'/',i,'.grd'))
  raster = raster+species_raster    
  }
}

library(sf)
world_map = st_read('../../spatial_r_workshop/data/global/ne_50m_land/ne_50m_land.shp')
transformed_world = st_transform(world_map,projection(raster))
world_spatial <- as(transformed_world, 'Spatial')
diversity_raster = mask(raster,world_spatial)
plot(diversity_raster,main='Carnivora diversity')

```


To get a better understanding of your species diversity raster, just print the raster object to screen :

```{r, message=FALSE, warning=FALSE, echo=FALSE}
diversity_raster
```


Check the dimension of you raster using the `dim()` command, which tells you how many cells the raster contains along the x and y axis.

```{r, message=FALSE, warning=FALSE}
dim(diversity_raster)
```

You can get an idea of the coordinate system your raster data is stored in by checking the extent of the raster:

```{r, message=FALSE, warning=FALSE}
extent(diversity_raster)
```

This is how you check the projection of the cluster:

```{r, message=FALSE, warning=FALSE}
projection(diversity_raster)
```

Note that the mammal diversity data is in CEA format 


### Predictor data

Now it's time to load the data for the predictor variables, which are the factors we want to test for correlation with mammalian species diversity. You can download whatever variables you want to check (for example get annual precitipitation data if you want to test if precipitation is a predictor of species diversity). A lot of useful climatic data can be found at <http://chelsa-climate.org/bioclim/>, but feel free to use any data source you want to. In this example I will work with annual average precipitation grid data, but you should preferably chose a different variable to test:

```{r, message=FALSE, warning=FALSE}
precipitation_file = '../personal/data/CHELSA_bio10_12.tif'
precipitation_raster = raster(precipitation_file)
plot(precipitation_raster,main='Precipitation')
```

It's generally good practice to log-transform absolute precipitation values, since they spread over several orders of magnitudes. That's done like this:

```{r, message=FALSE, warning=FALSE, cache=TRUE}
values(precipitation_raster) = log(values(precipitation_raster))
```

If you are using bioclim data you'll see that the resolution of the raster is incredibly high, which makes for very slow processing times for all operations on the raster. Let's therefore first reduce the data to a lower resolution.

If you want to rescale your raster to a coarser resolution you can use the `aggregate()` function. Say you want to change the resolution of your raster to roughly 0.1 grid size (approx. 10x10km cell-size at the equator), you can rescale the raster by factor `0.1/res(precipitation_raster)`. Note that the aggregate function only accepts integer values. If a factor is provided that is not an integer but a float, the function will round it to the closest integer value. This command may take around 10 minutes to finish.

```{r, message=FALSE, warning=FALSE}
# round the values of the extent of this raster to integers
extent(precipitation_raster) = round(extent(precipitation_raster))
precipitation_raster_reduced = aggregate(precipitation_raster, fact = 0.1/res(precipitation_raster))
precipitation_raster_reduced
```

We need to make sure that our predictor data (precipitation) is in the same projection as the response variable data (species diversity).

It makes most sense to project all data into the CEA projection, since this will optimize the cells towards having a close to equal area. If we wouldn't optimize for equal area but instead use the Mercator projection (lon-lat), our species diversity values would be biased toward the equator where cells would be bigger than toward the poles and would therefore inflate the values of species diversity the bigger the cell is. Using the CEA projection instead minimizes this bias. However some bias remains as it is impossible to project a globe into perfectly equal sized grid cells.

We try to further optimize the size-equality of our cells by picking the reference point of latitude=30 for our projection. The further a grid cell is away from this latitude, the smaller it's area is going to be. Since most of the global landmass is in the Northern hemisphere, picking this latitude is usually the best solution for global analyses. If however you find yourself working on spatial data only pertaining to South America, you may consider picking a different latitude (e.g. the equator at latitude=0). The way to code our desired projection (CEA with centered at latitude=30) is `"+proj=cea +datum=WGS84 +lat_ts=30"`.

```{r, message=FALSE, warning=FALSE}
precipitation_data_cea = projectRaster(precipitation_raster_reduced,crs="+proj=cea +datum=WGS84 +lat_ts=30")
precipitation_data_cea
```

To demonstrate the difference between the two projections, plot the raster in both projections and compare e.g. the size of Greenland in relation to areas around the equator:

```{r, message=FALSE, warning=FALSE}
plot(precipitation_raster_reduced,main='Mercator')
```

```{r, message=FALSE, warning=FALSE}
plot(precipitation_data_cea,main='CEA')
```


Also, notice how the resolution of your raster has changed too by transforming it into another projection. This is expected because the cells of the raster are being restructured and altered in size when changing from one projection into another.

```{r, message=FALSE, warning=FALSE}
print(dim(precipitation_raster_reduced))
print(dim(precipitation_data_cea))
```


### Matching the spatial data

Now we need to make sure that we have the exact same number of cells (rows and columns) for our response variable and predictor variables.

```{r, message=FALSE, warning=FALSE}
print(dim(diversity_raster))
print(dim(precipitation_data_cea))
```

We can use the `resample()` function to match the dimensions of one raster to those of another:

```{r, message=FALSE, warning=FALSE}
precipitation_data_matched = resample(precipitation_data_cea, diversity_raster, method='bilinear')
```

Let's see if it worked:

```{r, message=FALSE, warning=FALSE}
print(dim(diversity_raster))
print(dim(precipitation_data_matched))
```

For faster processing in the following, let us reduce the resolution of the rasters some more, say by factor 3:

```{r, message=FALSE, warning=FALSE}
#final_diversity_raster = aggregate(diversity_raster,factor=3)
#final_precipitation_raster = aggregate(precipitation_data_matched,factor=3)
final_diversity_raster = diversity_raster
final_precipitation_raster = precipitation_data_matched
```



### Fitting linear models

```{r, message=FALSE, warning=FALSE}
global_temp = read.csv('../data/global_temp.txt',sep='\t')
co2 = read.csv('../data/co2.txt',sep='\t')
plot(co2$co2,global_temp$Temperature, pch = 16, xlab = "co2", ylab = "Temperature")
```

```{r, message=FALSE, warning=FALSE}
combined = cbind(global_temp,co2)
model <- glm(Temperature ~ co2, family = gaussian ,data = combined)
xweight <- seq(range(co2$co2)[1], range(co2$co2)[2], 0.01)
yweight <- predict(model, list(co2 = xweight),type="response")
plot(co2$co2,global_temp$Temperature, pch = 16, xlab = "co2", ylab = "Temperature")
lines(xweight, yweight,col='red')
```


### Temporal auto-correlation

*Disclaimer: Some of the following examples are borrowed from a great tutorial on spatial auto-correlation at <https://rspatial.org/raster/analysis/3-spauto.html>.*

Before we get into spatial auto-correlation in particular, let's first talk about auto-correlation in general. If your data are auto-correlated, it means that they are not independent data points but that they are correlated to some extent. When using auto-correlated data without correcting for the correlation, one usually greatly overestimates the effective sample size.

For example if you want to measure a person's weight through time, you would expect two measurements which are close to each other in time to also be similar in the measured variable. To measure the degree of association over time, we can compute the correlation of each observation with the next observation.

Check for temporal auto-correlation. We are going to compute the "one-lag" auto-correlation, which means that we compare each value to its immediate neighbour, and not to other nearby values.

```{r, message=FALSE, warning=FALSE}
values <- co2$co2
a <- values[-length(values)]
b <- values[-1]
print(cor(a,b))
```

```{r, message=FALSE, warning=FALSE}
values <- global_temp$Temperature
a <- values[-length(values)]
b <- values[-1]
cor(a,b)
```

These values indicate a very strong positive temporal auto-correlation for the "one-lag" method. There are also integrated and more elegant ways in R for determining auto-correlation than our manual implementation of the "one-lag" auto-correlation. For example the `acf()` function computes the autocorrelation for several lags:

```{r, message=FALSE, warning=FALSE}
acf(co2$co2)
```

```{r, message=FALSE, warning=FALSE}
acf(global_temp$Temperature)
```

We see that both global temperature and CO2 content of the atmosphere each show a strongly positive temporal auto-correlation. In this case it is caused by a clear temporal trend in the data, since CO2 levels are increasing with time and the temparature in reponse increases as well. However, temporal auto-correlation could also occur if data points of "neighbouring" years influence each other, without there being an overall trend.


### Spatial auto-correlation

Similar to temporal auto-correlation, spatial auto-correlation means that two points or raster cells that are close to each other in space have similar values. In our example, in case our raster cells are spatially auto-correlated, we expect the species diversity values of two neighbouring cells to be more similar to each other than to further away cells (on average).

This auto-correlation can be exogenous (caused by some unknown/untested effects that effect neighbouring cells in a similar manner) or endogenous (caused by the variable we're testing, e.g. temperature).

In the following we're trying to quantify the degree to which neighbouring raster cells are similar to each other (more specifically we're determining how similar their [residuals](https://www.statisticshowto.datasciencecentral.com/residual/) are), using different definitions/thresholds of "neighbourhood". We then include the determined autocorrelation into our model in order to account for the sum of the exogenous auto-correlation casued by unknown factors. Only by accounting for this can we measure the true effect of our tested variables on species diversity.

But before getting into neighbourhoods etc. let us first fit a general linear model to our species diversity data, without worrying about spatial auto-correlation and neighbourhoods.

First we need to bring our raster data into a dataframe format. For this we extract the coordinates (raster cell centroids) using the `coordinates()` function. The coordinates should be identical for both of our rasters, check if that is the case by repeating the below for your raster of predictor values. You should get the same coordinates and number of points for both rasters.

```{r, message=FALSE, warning=FALSE, cache=TRUE}
coordinates = coordinates(final_diversity_raster)
```

Now get the corresponding values from both rasters, using the `values()` function. Then we put the coordinates and all raster values together into one dataframe:

```{r, message=FALSE, warning=FALSE}
species_div = values(final_diversity_raster)
precipitation = values(final_precipitation_raster)
all_data_merged = as.data.frame(cbind(coordinates,species_div,precipitation))
```

There are a lot of NA values resulting from the water cells in the two rasters, which didn't have any values assigned to them. We need to remove those before continuing to work with the data. Therefore let's just remove all rows from the dataframe that contain NA's using the `complete.cases()` function:

```{r, message=FALSE, warning=FALSE}
final_data = all_data_merged[complete.cases(all_data_merged),]
```

Further it is good practice to scale the values for both the response and the predictor variables to be centered in 0 using the `scale()` function:

```{r, message=FALSE, warning=FALSE}
final_data$species_div=scale(final_data$species_div)[,1]
final_data$precipitation=scale(final_data$precipitation)[,1]
```


#### General linear model (GLM)

Now plot the final values of the predictor we want to test against the response variable to get a first impression on their relationship:

```{r, message=FALSE, warning=FALSE}
plot(final_data$precipitation,final_data$species_div, pch = 16, xlab = "Precipitation", ylab = "Carnivora species diversity")
```

For faster computation, we'll go through the following steps just using a subsample of the data. Note that eventually you need to run the final model using all of the data, but for now let's stick to the subsample.

```{r, message=FALSE, warning=FALSE}
set.seed(42)
# take a random sample of 500 points from the dataframe
subsample = final_data[sample(nrow(final_data), 500), ]
```

Now fit the linear model to the data and plot the results:

```{r, message=FALSE, warning=FALSE}
glm_model = glm(species_div~precipitation,data=subsample)
xweight = seq(range(subsample$precipitation)[1], range(subsample$precipitation)[2], 0.1)
yweight = predict(glm_model, list(precipitation = xweight),type="response")
plot(subsample$precipitation,subsample$species_div, pch = 16, xlab = "Precipitation", ylab = "Carnivora species diversity")
lines(xweight, yweight,col='red')
```


You can check how strong your tested predictor affects the response variable by using the `summary()` command on the model:

```{r, message=FALSE, warning=FALSE}
summary(glm_model)
```

In the `Coefficients` section you can find the effect size of your predictor (value after the respective predictor variable name). Also you can see how significant the effect is by looking at the p-value (`Pr(>|t|)`). 


Above in the temporal auto-correlation example we determined the auto-correlation at different lags using the `acf()` function. For spatial data we can use the `correlog()` function which calculates the Moran's I (as mentioned in the introduction slides) for different distances of points. This is a measure of the spatial auto-correlation of the residuals at different distances.

```{r, message=FALSE, warning=FALSE}
library(ncf)
autocorrelation_glm = correlog( subsample$x, subsample$y, glm_model$residuals, increment=1000, latlon=T, resamp=100)
plot(autocorrelation_glm)
```

You can see that for close distances (< 4000 km) we find a strong positive spatial auto-correlation. This is a common pattern for spatial data and is the reason why we need to bother with accounting for this auto-correlation in our model. 



#### Linear models including neighbouthoods (SAR model)

In order to formalize this spatial auto-correlation and integrate it into our linear model, we need some measure of neighbourhood. First step is to transform our coordinates back into lat-lon format, since this is required for properly calculating the neighbourhoods distances.

```{r, message=FALSE, warning=FALSE}
coordinates = cbind(subsample$x,subsample$y)
coordinates_sp = SpatialPoints(coordinates,proj4string = CRS(projection(final_diversity_raster)))
coordinates_transformed = spTransform(coordinates_sp, CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
coordinates_df = as.data.frame(coordinates_transformed)
subsample$x = coordinates_df$coords.x1
subsample$y = coordinates_df$coords.x2
```


Now we can use the `knearneigh()` function to identify the n closest neighbouring cells for each given cell. In the example below we extract the 2 closest neighbours for each cell. Note the argument `longlat = T` which tells the function that the input data is in lon-lat format:

```{r, message=FALSE, warning=FALSE}
library(spdep)

nearest_neighbours_2 = knearneigh(cbind(subsample$x, subsample$y),2,longlat = T)
# This function sorts out the spatial elements
neighbourlist_2_closests = knn2nb(nearest_neighbours_2)
# This illustrates what we have done
plot(neighbourlist_2_closests, cbind(subsample$x, subsample$y))
```

This looks very abstract, but you can roughly see the continents or islands the points in our random subsample were drawn from and the two closest neighbours of each point connected by lines.

Now we will apply the SAR model (`errorsarlm()`), which allows us to incorporate the neighbourhood of points. The formula of the SAR model is a modification of the general linear model (`y = X beta + u, u = lambda W u + e`), adding the error term `u`. The `W` in this error term are our modeled neighbourhood relationships, which are parsed to the function using the `listw()` argument.

```{r, message=FALSE, warning=FALSE}
sar_model = errorsarlm(species_div~precipitation, data=subsample, listw=nb2listw(neighbourlist_2_closests), tol.solve = 1e-12, zero.policy =T)
autocorrelation_sar = correlog(subsample$x, subsample$y, sar_model$residuals, increment=1000, latlon=T, resamp=100)
plot(autocorrelation_sar)
```


You can see that accounting for the two closest neighbours has already improved our results. While the shape of the correlogram still looks somewhat similar (higher spatial auto-correlation at small distances), the magnitude of the values has changed dramatically (from max ~ 0.8 to max ~ 0.15), which means that the measured autocorrelation is much smaller than in the GLM case above (general linear model) where we don't account for spatial auto-correlation.


Let's also check the model summary:

```{r, message=FALSE, warning=FALSE}
summary(sar_model)
```

We can see that the effect size of our predictor variable is much smaller than for the GLM model. This is expected because the predictor effect size in the GLM does not distinguish between the actual predictor and the effect size based on auto-correlation. The effect size of the SAR model on the other hand describes the true effect size of the predictor, not including biases caused by spatial-autocorrelation.

However we can improve this model further by testing different definitions of neighbourhood. Instead of calculating the distance to the n closest neighbours with the `knearneigh()` function, we can instead use the `dnearneigh()` function, which is based on a distance threshold and extracts all points within that distance. For example the following command extracts all points that are within a distance of 1500km of each point:

```{r, message=FALSE, warning=FALSE}
neighbours_1500km = dnearneigh(cbind(subsample$x,subsample$y), 0,1500, longlat = T)
# This illustrates what we have done
plot(neighbours_1500km, cbind(subsample$x, subsample$y))
```

You see that the neighbourhoods look very different, since only points within 1500 km are connected. This also leads to some points having no neighbours at all, since they are too isolated. Also, note that the longer lines going across the whole map are a result of us using lon-lat input data (remember, the Earth is a globe).

Let's see how this neighbourhood definition performs compared to the n closests model from before. Note: The `zero.policy =T` argument in the SAR model and in the `nb2listw()` function is necessary to account for points without neighbours.

```{r, message=FALSE, warning=FALSE}
sar_model = errorsarlm(species_div~precipitation, data=subsample, listw=nb2listw(neighbours_1500km,zero.policy =T), tol.solve = 1e-12, zero.policy =T)
autocorrelation_sar = correlog(subsample$x, subsample$y, sar_model$residuals, increment=1000, latlon=T, resamp=100)
plot(autocorrelation_sar)
```

The results don't look too different to before.

Let's also check the model summary:

```{r, message=FALSE, warning=FALSE}
summary(sar_model)
```

Generally one should test a bunch of different configurations of the `knearneigh()` and `dnearneigh()` neighbourhoods, using different numbers of neighbours and different distances respectively. After testing different configurations the best model can be selected using a model selection criterion such as AIC. For that purpose we first define a function that runs through all our desired model configurations. Note: one can also try different values for the `style=` argument, common options are `B`,`C`,`U`,`S`, or `W` as used in the example below:

```{r, message=FALSE, warning=FALSE}
Neighborhood_generator=function(COOR) {
models<<-list(
  nb2listw(knn2nb(knearneigh(COOR,1, longlat = T)),style="W",zero.policy =T),
  nb2listw(knn2nb(knearneigh(COOR,2, longlat = T)),style="W",zero.policy =T),
  nb2listw(knn2nb(knearneigh(COOR,3, longlat = T)),style="W",zero.policy =T),
  nb2listw(knn2nb(knearneigh(COOR,4, longlat = T)),style="W",zero.policy =T),
  nb2listw(knn2nb(knearneigh(COOR,5, longlat = T)),style="W",zero.policy =T),
  nb2listw(knn2nb(knearneigh(COOR,6, longlat = T)),style="W",zero.policy =T),
  nb2listw(knn2nb(knearneigh(COOR,7, longlat = T)),style="W",zero.policy =T),
  nb2listw(knn2nb(knearneigh(COOR,8, longlat = T)),style="W",zero.policy =T),
  nb2listw(knn2nb(knearneigh(COOR,9, longlat = T)),style="W",zero.policy =T),
  nb2listw(knn2nb(knearneigh(COOR,10, longlat = T)),style="W",zero.policy =T),
  nb2listw(dnearneigh(COOR, 0,250, longlat = T),style="W",zero.policy =T),
  nb2listw(dnearneigh(COOR, 0,500, longlat = T),style="W",zero.policy =T),
  nb2listw(dnearneigh(COOR, 0,750, longlat = T),style="W",zero.policy =T),
  nb2listw(dnearneigh(COOR, 0,1000, longlat = T),style="W",zero.policy =T),
  nb2listw(dnearneigh(COOR, 0,1250, longlat = T),style="W",zero.policy =T),
  nb2listw(dnearneigh(COOR, 0,1500, longlat = T),style="W",zero.policy =T),
  nb2listw(dnearneigh(COOR, 0,2000, longlat = T),style="W",zero.policy =T),
  nb2listw(dnearneigh(COOR, 0,2500, longlat = T),style="W",zero.policy =T),
  nb2listw(dnearneigh(COOR, 0,3000, longlat = T),style="W",zero.policy =T),
  nb2listw(dnearneigh(COOR, 0,3500, longlat = T),style="W",zero.policy =T),
  nb2listw(dnearneigh(COOR, 0,4000, longlat = T),style="W",zero.policy =T)
)
}
```


Now we apply that function to our coordinates of the random subsample of our data:

```{r, message=FALSE, warning=FALSE}
neighbourhood_models = Neighborhood_generator(cbind(subsample$x,subsample$y))
```

Calculate the AICc score for each neighbourhood model using the `AICc()` function. This is a version of the regular AIC criterion that additionally corrects for small sample size:
```{r, message=FALSE, warning=FALSE}
library(wiqid)

AIC_LIST=numeric(length(neighbourhood_models))
for (i in 1:length(neighbourhood_models)) {
	AIC_LIST[i]=AICc(errorsarlm(species_div~precipitation, data=subsample,listw=neighbourhood_models[[i]], tol.solve = 1e-12, zero.policy =T))
}
```

Select the model with the lowest AIC score as the best model:

```{r, message=FALSE, warning=FALSE}
index_best_model = which(AIC_LIST==min(AIC_LIST))
best_neighbour_model = neighbourhood_models[index_best_model]
```

Plot the correlogram:

```{r, message=FALSE, warning=FALSE}
sar_model = errorsarlm(species_div~precipitation, data=subsample, listw=best_neighbour_model[[1]], tol.solve = 1e-12, zero.policy =T)
autocorrelation_sar = correlog(subsample$x, subsample$y, sar_model$residuals, increment=1000, latlon=T, resamp=100)
plot(autocorrelation_sar)
```

This looks quite good, since the auto-correlation at small distances has completely disappeared. There still seems to be some evidence of auto-correlation at very large distances (>16,000km), but this is quite normal and nothing to worry about.

Let's check the model summary:

```{r, message=FALSE, warning=FALSE}
summary(sar_model)
```


#### Determine model fit

Besides using the `summary()` function to check the size of the effect of the predictor and the significance of it, you can get a measure of model fit by calculating the pseudo R-square value.

```{r, message=FALSE, warning=FALSE}
# This gives a pseudo R square for the glm
cor(predict(glm_model), subsample$species_div)^2
# This gives a pseudo R square for the SAR but for both the predictors and the neighborhood part
cor(predict(sar_model),  subsample$species_div)^2
```

The R-square value for the SAR model is much higher, which is expected because it contains the spatial auto-correlation information, which we saw is very strong in the data. Therefore this value does not tell us much about the predictive power of our actual predictor variable, but only of the whole model. To get an idea of the R-square for only the predictor we can re-predict the data, using the coefficients estimated by the model and putting them together to a linear formula (note that if you use multiple predictors in your model, each will have it's own coefficient and will need to be added to the formula):

```{r, message=FALSE, warning=FALSE}
predicted = c(sar_model$coefficients[1]+sar_model$coefficients[2]*subsample$precipitation)
cor(predicted, subsample$species_div)^2
```

In this case the pseudo R-square value of the final model is the same as that of the GLM model. Howevere, this will change once you add multiple predictors into the equation.


**Take-home tasks:**

- Run the selected best model on the full dataset (instead of the subsample we used above)

- Add multiple predictors (at least 2) in one joined analysis. The general syntax to include multiple predictors into the model is e.g. `glm(species_div~predictor1+predictor2,data=subsample)`, or if you want to include interactions between predictors `glm(species_div~predictor1*predictor2,data=subsample)` (equivalent syntax for the SAR model).


